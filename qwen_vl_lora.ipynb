{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485df37",
   "metadata": {},
   "source": [
    "# 在 Azure Machine Learning 上使用 LoRA 微调 Qwen-VL\n",
    "\n",
    "本笔记演示如何在 Azure Machine Learning (Azure ML) 上对 Qwen-VL 系列多模态大模型执行 LoRA 指令微调，并在训练完成后注册模型及验证推理结果。流程涵盖环境配置、数据准备、训练脚本编写、作业提交与监控等关键步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c68b",
   "metadata": {},
   "source": [
    "## 前提条件\n",
    "\n",
    "- Azure 订阅以及已创建的 Azure ML 工作区。\n",
    "- 工作区所在区域已经申请到足够的 GPU 配额（推荐 `Standard_NC24ads_A100_v4`）。\n",
    "- 本地或计算实例已安装 `azure-ai-ml>=1.15.0` 与 `azure-identity>=1.16.0`。\n",
    "- 若使用托管身份或服务主体，请确保具备对工作区的访问权限。\n",
    "- 准备图文对训练数据，推荐整理为 JSON Lines 格式（示例见后文）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d582b",
   "metadata": {},
   "source": [
    "## 操作文档概览\n",
    "\n",
    "执行本指南时建议按以下顺序完成：\n",
    "1. 在本地整理数据（图片 + 标注 JSONL），并根据示例检查字段。\n",
    "2. 将数据上传/注册到 Azure ML 数据存储或数据资产。\n",
    "3. 检查或创建 GPU 计算集群。\n",
    "4. 运行 Notebook 前半部分生成并注册训练环境、上传脚本。\n",
    "5. 根据实际资源修改命令作业参数并提交训练。\n",
    "6. 监控日志、下载输出，最后注册模型并本地验证。\n",
    "7. （可选）基于注册模型继续部署或批量推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ab1b6",
   "metadata": {},
   "source": [
    "## 连接到 Azure ML 工作区\n",
    "\n",
    "填写订阅 ID、资源组与工作区名称。以下示例优先使用 `DefaultAzureCredential`，若本地无可用身份则回退到交互式浏览器登录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6815208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to workspace: aml-rg\n"
     ]
    },
    {
     "ename": "HttpResponseError",
     "evalue": "(AuthorizationFailed) The client 'chengbian@microsoft.com' with object id '35c5cf33-0c22-4334-95da-4cf7384b5260' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/7a03e9b8-18d6-48e7-b186-0ec68da9e86f/resourceGroups/aml-hu-east-west-us2/providers/Microsoft.MachineLearningServices/workspaces/aml-rg' or the scope is invalid. If access was recently granted, please refresh your credentials.\nCode: AuthorizationFailed\nMessage: The client 'chengbian@microsoft.com' with object id '35c5cf33-0c22-4334-95da-4cf7384b5260' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/7a03e9b8-18d6-48e7-b186-0ec68da9e86f/resourceGroups/aml-hu-east-west-us2/providers/Microsoft.MachineLearningServices/workspaces/aml-rg' or the scope is invalid. If access was recently granted, please refresh your credentials.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     14\u001b[39m ml_client = MLClient(\n\u001b[32m     15\u001b[39m     credential=credential,\n\u001b[32m     16\u001b[39m     subscription_id=SUBSCRIPTION_ID,\n\u001b[32m     17\u001b[39m     resource_group_name=RESOURCE_GROUP,\n\u001b[32m     18\u001b[39m     workspace_name=WORKSPACE_NAME,\n\u001b[32m     19\u001b[39m  )\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnected to workspace: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mml_client.workspace_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m workspace = \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspaces\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 读取工作区以获取区域信息\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLocation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkspace.location\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_workspace_operations.py:141\u001b[39m, in \u001b[36mWorkspaceOperations.get\u001b[39m\u001b[34m(self, name, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@monitor_with_activity\u001b[39m(ops_logger, \u001b[33m\"\u001b[39m\u001b[33mWorkspace.Get\u001b[39m\u001b[33m\"\u001b[39m, ActivityType.PUBLICAPI)\n\u001b[32m    121\u001b[39m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# pylint: disable=arguments-renamed\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Dict) -> Optional[Workspace]:\n\u001b[32m    124\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a Workspace by name.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[33;03m    :param name: Name of the workspace.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m \u001b[33;03m            :caption: Get the workspace with the given name.\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/ai/ml/operations/_workspace_operations_base.py:89\u001b[39m, in \u001b[36mWorkspaceOperationsBase.get\u001b[39m\u001b[34m(self, workspace_name, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m workspace_name = \u001b[38;5;28mself\u001b[39m._check_workspace_name(workspace_name)\n\u001b[32m     88\u001b[39m resource_group = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mresource_group\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._resource_group_name\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m v2_service_context = {}\n\u001b[32m     92\u001b[39m v2_service_context[\u001b[33m\"\u001b[39m\u001b[33msubscription_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._subscription_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/qwen_vl_lora/.venv/lib/python3.12/site-packages/azure/ai/ml/_restclient/v2024_10_01_preview/operations/_workspaces_operations.py:957\u001b[39m, in \u001b[36mWorkspacesOperations.get\u001b[39m\u001b[34m(self, resource_group_name, workspace_name, **kwargs)\u001b[39m\n\u001b[32m    955\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m    956\u001b[39m     error = \u001b[38;5;28mself\u001b[39m._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\u001b[32m    959\u001b[39m deserialized = \u001b[38;5;28mself\u001b[39m._deserialize(\u001b[33m'\u001b[39m\u001b[33mWorkspace\u001b[39m\u001b[33m'\u001b[39m, pipeline_response)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[31mHttpResponseError\u001b[39m: (AuthorizationFailed) The client 'chengbian@microsoft.com' with object id '35c5cf33-0c22-4334-95da-4cf7384b5260' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/7a03e9b8-18d6-48e7-b186-0ec68da9e86f/resourceGroups/aml-hu-east-west-us2/providers/Microsoft.MachineLearningServices/workspaces/aml-rg' or the scope is invalid. If access was recently granted, please refresh your credentials.\nCode: AuthorizationFailed\nMessage: The client 'chengbian@microsoft.com' with object id '35c5cf33-0c22-4334-95da-4cf7384b5260' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/7a03e9b8-18d6-48e7-b186-0ec68da9e86f/resourceGroups/aml-hu-east-west-us2/providers/Microsoft.MachineLearningServices/workspaces/aml-rg' or the scope is invalid. If access was recently granted, please refresh your credentials."
     ]
    }
   ],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "import os\n",
    "\n",
    "# 读取工作区基本信息，优先从环境变量中获取\n",
    "SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\", \"7a03e9b8-18d6-48e7-b186-0ec68da9e86f\")\n",
    "RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\", \"aml-hu-east-west-us2\")\n",
    "WORKSPACE_NAME = os.getenv(\"AZUREML_WORKSPACE_NAME\", \"aml-rg\")\n",
    "\n",
    "# 交互式登录\n",
    "credential = InteractiveBrowserCredential()\n",
    "\n",
    "# 初始化 MLClient 以便后续操作工作区资源\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    " )\n",
    "\n",
    "print(f\"Connected to workspace: {ml_client.workspace_name}\")\n",
    "workspace = ml_client.workspaces.get(ml_client.workspace_name)  # 读取工作区以获取区域信息\n",
    "print(f\"Location: {workspace.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaee20e",
   "metadata": {},
   "source": [
    "## 准备目录结构与环境文件\n",
    "\n",
    "在工作目录下创建源码与环境配置文件夹，并写入将用于训练作业的 Conda 依赖。环境基于 Azure ML 官方 PyTorch CUDA 12.1 镜像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8903c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# 在本地构建源码和环境目录，用于组织训练脚本与依赖文件\n",
    "workspace_dir = pathlib.Path.cwd() / \"qwen_vl_lora\"\n",
    "src_dir = workspace_dir / \"src\"\n",
    "env_dir = workspace_dir / \"env\"\n",
    "workspace_dir.mkdir(exist_ok=True)\n",
    "src_dir.mkdir(parents=True, exist_ok=True)\n",
    "env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Source directory: {src_dir}\")\n",
    "print(f\"Environment directory: {env_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {env_dir}/conda.yaml\n",
    "# Conda 环境描述，Azure ML 会据此在基础镜像上安装额外依赖\n",
    "name: qwen-vl-lora-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - accelerate>=0.28.0\n",
    "    - bitsandbytes>=0.43.0\n",
    "    - datasets>=2.18.0\n",
    "    - peft>=0.11.0\n",
    "    - pillow>=10.2.0\n",
    "    - sentencepiece>=0.2.0\n",
    "    - timm>=0.9.12\n",
    "    - torch>=2.3.0\n",
    "    - torchvision>=0.18.0\n",
    "    - transformers>=4.39.0\n",
    "    - trl>=0.8.6\n",
    "    - wandb>=0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# 将自定义 Conda 依赖与官方基础镜像组合成 Azure ML 环境\n",
    "qwen_env = Environment(\n",
    "    name=\"qwen-vl-lora-env\",\n",
    "    description=\"LoRA finetuning environment for Qwen-VL\",\n",
    "    conda_file=str((env_dir / \"conda.yaml\").as_posix()),\n",
    "    image=\"mcr.microsoft.com/azureml/curated/pytorch-2.3-cuda12.1-cudnn8-ubuntu20.04:latest\",\n",
    ")\n",
    "registered_env = ml_client.environments.create_or_update(qwen_env)\n",
    "print(f\"Environment registered: {registered_env.name}:{registered_env.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314ef2f",
   "metadata": {},
   "source": [
    "### 计算资源准备\n",
    "1. 推荐使用带 A100 80GB 的 GPU（如 `Standard_NC24ads_A100_v4`）。\n",
    "2. 若尚未创建计算集群，可在 Azure ML Studio → 计算 → 计算集群 创建，或使用 Azure CLI：\n",
    "   ```bash\n",
    "   az ml compute create --name gpu-a100-cluster --type amlcompute \\\n",
    "       --resource-group <RG> --workspace-name <WS> \\\n",
    "       --min-instances 0 --max-instances 2 --size Standard_NC24ads_A100_v4\n",
    "   ```\n",
    "3. 训练前确认集群处于“空闲”或“已分配”状态，确保配额足够。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c355894",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "\n",
    "> 目标：整理图文多轮对话数据，并转换为 LoRA 训练所需的 JSON Lines 文件。\n",
    "\n",
    "### 1. 本地目录结构建议\n",
    "- `dataset/`\b根目录\n",
    "  - `images/`\b存放所有训练图片，可按需要再分子目录。\n",
    "  - `train.jsonl`\b训练集标注文件。\n",
    "  - `validation.jsonl`\b验证集（可选）。\n",
    "\n",
    "### 2. JSONL 单条样本示例\n",
    "```json\n",
    "{\n",
    "  \"image\": \"images/sample_0001.jpg\",\n",
    "  \"question\": \"描述图片中的主要场景。\",\n",
    "  \"answer\": \"图片展示了一位骑行者在海边公路上骑行，阳光明媚。\",\n",
    "  \"system\": \"你是一名图文理解助手。\"\n",
    "}\n",
    "```\n",
    "- `image`\b可以是相对路径（相对于数据集根目录）或绝对路径；\n",
    "- `question`\b为用户指令/提问；\n",
    "- `answer`\b为模型期望回应；\n",
    "- `system`\b可选，用于提供系统提示或角色设定；\n",
    "- 可扩展额外字段（如标签、难度）但需在训练脚本中自行处理。\n",
    "\n",
    "### 3. 将其他标注格式转为 JSONL\n",
    "若已有 CSV/Excel/JSON，可在本地运行以下示例脚本生成 `train.jsonl`：\n",
    "```python\n",
    "import csv, json, pathlib\n",
    "\n",
    "input_csv = pathlib.Path(\"./raw.csv\")\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "\n",
    "with input_csv.open(\"r\", encoding=\"utf-8\") as fin, output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for row in reader:\n",
    "        record = {\n",
    "            \"image\": f\"images/{row['image_filename']}\",\n",
    "            \"question\": row[\"instruction\"],\n",
    "            \"answer\": row[\"response\"],\n",
    "            \"system\": row.get(\"system_prompt\") or \"你是一名视觉助手。\"\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "```\n",
    "脚本执行后确认：\n",
    "1. JSONL 每行均为合法 JSON；\n",
    "2. 引用的图片文件均存在且可打开；\n",
    "3. 验证集（若存在）与训练集结构一致即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec4705",
   "metadata": {},
   "source": [
    "转换完成后重复前述上传步骤，将新生成的 JSONL 文件和图片目录同步到 Azure ML 数据存储即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf1eca",
   "metadata": {},
   "source": [
    "### 5. 从 Parquet/Arrow 数据源生成 JSONL\n",
    "若你的训练数据保存在 `test-00000-of-00001.parquet` 等 Parquet 文件中，可按以下步骤转换：\n",
    "1. 使用 `datasets` 或 `pyarrow` 读取 Parquet；\n",
    "2. 逐行构建所需字段并写入 JSONL；\n",
    "3. 同时批量导出引用到的图片（若存储为二进制或远程 URL）。\n",
    "\n",
    "下面示例 Parquet 中包含字段 'id', 'category', 'images', 'question', 'question_text', 'answer', 'difficulty', 'metric_info', 'initial_state'，并且 `images` 已指向本地图片："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e061c",
   "metadata": {},
   "source": [
    "如 Parquet 中包含嵌入二进制图像，可先使用 `dataset[i][\"image\"].save(...)` 将其导出到 `images/` 目录，再在生成 JSONL 时写入相对路径。若 `image_path` 为远程 URL，需提前下载到本地后再写入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02885af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json, pathlib\n",
    "from PIL import Image\n",
    "\n",
    "parquet_path = \"./dataset/test-00000-of-00001.parquet\"  # 修改为你的 parquet 路径\n",
    "output_jsonl = pathlib.Path(\"./dataset/train.jsonl\")\n",
    "images_dir = pathlib.Path(\"./dataset/images\")\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=parquet_path, split=\"train\")\n",
    "\n",
    "def save_image_as_png(image: Image.Image, image_path: pathlib.Path) -> str:\n",
    "    \"\"\"统一将图片保存为 PNG，保持 Alpha 通道并避免压缩损失。\"\"\"\n",
    "    if image.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "        image = image.convert(\"RGBA\")\n",
    "    else:\n",
    "        image = image.convert(\"RGB\")\n",
    "    image.save(image_path, format=\"PNG\")\n",
    "    return f\"images/{image_path.name}\"\n",
    "\n",
    "def resolve_image_field(example, sample_idx: int) -> str:\n",
    "    \"\"\"将 parquet 中的 images 字段转换为 JSONL 所需的图片路径。\"\"\"\n",
    "    image_field = example.get(\"images\")\n",
    "    if isinstance(image_field, list):\n",
    "        image_field = image_field[0] if image_field else None\n",
    "    if isinstance(image_field, Image.Image):\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(image_field, image_path)\n",
    "    if hasattr(image_field, \"save\"):\n",
    "        pil_image = image_field\n",
    "        image_path = images_dir / f\"{sample_idx:06d}.png\"\n",
    "        return save_image_as_png(pil_image, image_path)\n",
    "    if isinstance(image_field, dict):\n",
    "        candidate = image_field.get(\"path\") or image_field.get(\"image_path\") or image_field.get(\"url\")\n",
    "        if candidate:\n",
    "            return candidate\n",
    "    if isinstance(image_field, str):\n",
    "        return image_field\n",
    "    raise ValueError(\"无法解析 parquet 中的 images 字段，请检查数据格式\")\n",
    "\n",
    "with output_jsonl.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, example in enumerate(dataset):\n",
    "        record = {\n",
    "            \"image\": resolve_image_field(example, idx),\n",
    "            \"question\": example.get(\"question_text\") or example.get(\"question\") or \"\",\n",
    "            \"answer\": example.get(\"answer\", \"\"),\n",
    "            \"system\": example.get(\"initial_state\") or \"你是一名视觉助手。\",\n",
    "            \"category\": example.get(\"category\"),\n",
    "            \"metadata\": {\"difficulty\": example.get(\"difficulty\"), \"metric_info\": example.get(\"metric_info\")},\n",
    "        }\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Wrote {len(dataset)} samples to {output_jsonl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f398f5b",
   "metadata": {},
   "source": [
    "### 4. 将本地数据上传到 Azure ML\n",
    "1. **确认默认数据存储**：运行下方代码查看 `workspaceblobstore` 或自定义数据存储的名称。\n",
    "2. **上传/注册数据资产**：可使用 Notebook 中的 Python 代码，也可通过 Azure ML Studio 的“数据”页面手动上传。\n",
    "3. **保持目录结构**：上传时确保 `images/` 等子目录与 JSONL 文件保持原有层级。\n",
    "4. **数据更新**：若重新上传，可选择新版本号，避免覆盖历史数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看当前工作区内可用的数据存储（默认通常为 workspaceblobstore）\n",
    "for ds in ml_client.datastores.list():\n",
    "    print(ds.name, \"->\", ds.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ff4f0",
   "metadata": {},
   "source": [
    "> **提示**：如果数据量较大，可使用 `azcopy` 或 Azure Storage Explorer 先上传到 Blob，再注册为数据资产。下方示例适合直接从本地文件夹上传。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "# 将本地文件夹上传并注册为数据资产\n",
    "local_dataset_path = pathlib.Path(\"./dataset\")  # 修改为你的本地数据集路径\n",
    "if not local_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"本地数据目录不存在: {local_dataset_path}\")\n",
    "\n",
    "data_asset = Data(\n",
    "    name=\"qwen-vl-instruction-data\",\n",
    "    version=datetime.utcnow().strftime(\"%Y%m%d%H%M\"),\n",
    "    path=local_dataset_path.as_posix(),\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"Qwen-VL LoRA instruction dataset\",\n",
    ")\n",
    "\n",
    "registered_data = ml_client.data.create_or_update(data_asset)\n",
    "print(f\"Data asset created: {registered_data.name}:{registered_data.version}\")\n",
    "print(f\"Asset path: {registered_data.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118ff8e",
   "metadata": {},
   "source": [
    "运行成功后，记下 `Data asset created` 输出的 ID/路径，并将其填写到后续 `DATASET_PATH` 变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {src_dir}/train_lora.py\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForVision2Seq,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    )\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    \"\"\"解析命令行参数，便于在 Azure ML 作业中灵活传参。\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"LoRA finetuning for Qwen-VL\")\n",
    "    parser.add_argument(\"--model-name\", type=str, default=\"Qwen/Qwen2-VL-7B-Instruct\")\n",
    "    parser.add_argument(\"--dataset-dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train.jsonl\")\n",
    "    parser.add_argument(\"--validation-file\", type=str, default=None)\n",
    "    parser.add_argument(\"--output-dir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--per-device-train-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--per-device-eval-batch-size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=8)\n",
    "    parser.add_argument(\"--num-train-epochs\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--warmup-ratio\", type=float, default=0.03)\n",
    "    parser.add_argument(\"--logging-steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--save-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--eval-strategy\", type=str, default=\"epoch\")\n",
    "    parser.add_argument(\"--lora-rank\", type=int, default=64)\n",
    "    parser.add_argument(\"--lora-alpha\", type=int, default=128)\n",
    "    parser.add_argument(\"--lora-dropout\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--target-modules\", type=str, default=\"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\")\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--trust-remote-code\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--report-to\", type=str, default=\"none\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def load_records(dataset_dir: str, file_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"读取 JSONL 文件，并返回样本列表。\"\"\"\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found: {file_path}\")\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    logger.info(\"Loaded %d samples from %s\", len(records), file_path)\n",
    "    return records\n",
    "\n",
    "def resolve_image_path(dataset_dir: str, image_path: str) -> str:\n",
    "    \"\"\"统一处理相对路径，便于在 Azure ML 计算节点上访问图片。\"\"\"\n",
    "    return image_path if os.path.isabs(image_path) else os.path.join(dataset_dir, image_path)\n",
    "\n",
    "@dataclass\n",
    "class QwenRecord:\n",
    "    \"\"\"用 dataclass 存储单条样本，提升可读性。\"\"\"\n",
    "    image: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    system: Optional[str] = None\n",
    "\n",
    "class QwenVLDataset(Dataset):\n",
    "    \"\"\"将原始 JSON 样本转换为模型可直接使用的提示格式。\"\"\"\n",
    "    def __init__(self, records: List[Dict[str, Any]], dataset_dir: str, processor: AutoProcessor):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.processor = processor\n",
    "        self.records: List[QwenRecord] = [\n",
    "            QwenRecord(\n",
    "                image=resolve_image_path(dataset_dir, item[\"image\"]),\n",
    "                question=item.get(\"question\", \"\"),\n",
    "                answer=item.get(\"answer\", \"\"),\n",
    "                system=item.get(\"system\")\n",
    "            )\n",
    "            for item in records\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        record = self.records[idx]\n",
    "        if not os.path.exists(record.image):\n",
    "            raise FileNotFoundError(f\"Image not found: {record.image}\")\n",
    "        image = Image.open(record.image).convert(\"RGB\")\n",
    "        messages: List[Dict[str, Any]] = []\n",
    "        if record.system:\n",
    "            messages.append({\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": record.system}]})\n",
    "        user_content: List[Dict[str, Any]] = [{\"type\": \"image\", \"image\": image}]\n",
    "        if record.question:\n",
    "            user_content.append({\"type\": \"text\", \"text\": record.question})\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": record.answer}]})\n",
    "        prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        return {\"prompt\": prompt, \"image\": image}\n",
    "\n",
    "class QwenDataCollator:\n",
    "    \"\"\"自定义 collator，将批次中的文本和图片一起编码为张量。\"\"\"\n",
    "    def __init__(self, processor: AutoProcessor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        prompts = [feature[\"prompt\"] for feature in features]\n",
    "        images = [feature[\"image\"] for feature in features]\n",
    "        batch = self.processor(\n",
    "            text=prompts,\n",
    "            images=images,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        return batch\n",
    "\n",
    "def create_model(args: argparse.Namespace) -> AutoModelForVision2Seq:\n",
    "    \"\"\"加载基础模型并应用 LoRA 配置，同时启用 4bit 量化以节省显存。\"\"\"\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        args.model_name,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.bfloat16 if args.bf16 else torch.float16,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "    target_modules = [module.strip() for module in args.target_modules.split(\",\") if module]\n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    processor = AutoProcessor.from_pretrained(args.model_name, trust_remote_code=args.trust_remote_code)\n",
    "    train_records = load_records(args.dataset_dir, args.train_file)\n",
    "    train_dataset = QwenVLDataset(train_records, args.dataset_dir, processor)\n",
    "    eval_dataset = None\n",
    "    if args.validation_file:\n",
    "        eval_records = load_records(args.dataset_dir, args.validation_file)\n",
    "        eval_dataset = QwenVLDataset(eval_records, args.dataset_dir, processor)\n",
    "    model = create_model(args)\n",
    "    collator = QwenDataCollator(processor)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        logging_steps=args.logging_steps,\n",
    "        save_strategy=args.save_strategy,\n",
    "        evaluation_strategy=\"no\" if eval_dataset is None else args.eval_strategy,\n",
    "        bf16=args.bf16 and torch.cuda.is_available(),\n",
    "        dataloader_num_workers=4,\n",
    "        report_to=[args.report_to] if args.report_to and args.report_to != \"none\" else [],\n",
    "        run_name=os.getenv(\"AML_RUN_ID\", \"qwen-vl-lora\"),\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer.train()\n",
    "    if eval_dataset is not None:\n",
    "        metrics = trainer.evaluate()\n",
    "        logger.info(\"Evaluation metrics: %s\", metrics)\n",
    "    trainer.save_model(args.output_dir)\n",
    "    processor.save_pretrained(os.path.join(args.output_dir, \"processor\"))\n",
    "    logger.info(\"Training completed. Artifacts saved to %s\", args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065aad3e",
   "metadata": {},
   "source": [
    "## 配置训练作业\n",
    "\n",
    "定义训练数据位置、输出目录、计算集群等参数。若数据存放在默认数据存储中，可直接使用 `azureml://datastores/<datastore>/paths/...` URI。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb80b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# 将数据路径替换为前一节注册成功的数据资产 ID 或数据存储 URI\n",
    "# 例如：DATASET_PATH = registered_data.id\n",
    "BASE_MODEL = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "DATASET_PATH = \"azureml://datastores/<your_datastore>/paths/qwen-vl-dataset/\"\n",
    "TRAIN_FILE = \"train.jsonl\"\n",
    "VALIDATION_FILE = \"validation.jsonl\"  # 若无验证数据可设置为 None\n",
    "OUTPUT_PATH = \"azureml://datastores/<your_datastore>/paths/qwen-vl-outputs/\"\n",
    "COMPUTE_NAME = \"gpu-a100-cluster\"\n",
    "EXPERIMENT_NAME = \"qwen-vl-lora\"\n",
    "\n",
    "train_input = Input(type=AssetTypes.URI_FOLDER, path=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0cf1a",
   "metadata": {},
   "source": [
    "## 创建并提交命令作业\n",
    "\n",
    "在运行下方代码前请确认：\n",
    "1. `DATASET_PATH`、`OUTPUT_PATH`、`COMPUTE_NAME` 等变量已替换为实际值；\n",
    "2. 训练脚本 `train_lora.py` 已根据需要调整超参数；\n",
    "3. 若要启用 WandB/MLflow 记录，请将 `--report-to` 设置为对应后端并配置凭据。\n",
    "\n",
    "随后执行代码即可提交 Azure ML 命令作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# 根据是否存在验证集动态拼接训练脚本命令\n",
    "validation_arg = f\"--validation-file {VALIDATION_FILE}\" if VALIDATION_FILE else \"\"\n",
    "command_parts = [\n",
    "    \"python train_lora.py\",\n",
    "    f\"--model-name {BASE_MODEL}\",\n",
    "    \"--dataset-dir ${inputs.data}\",\n",
    "    f\"--train-file {TRAIN_FILE}\",\n",
    "    \"--output-dir ./outputs\",\n",
    "    \"--per-device-train-batch-size 1\",\n",
    "    \"--gradient-accumulation-steps 8\",\n",
    "    \"--num-train-epochs 3\",\n",
    "    \"--learning-rate 2e-4\",\n",
    "    \"--logging-steps 5\"\n",
    "]\n",
    "if validation_arg:\n",
    "    command_parts.append(validation_arg)\n",
    "lora_command = \" \".join(command_parts)\n",
    "\n",
    "# 创建 Azure ML 命令作业，挂载数据并保存 LoRA 结果到指定目录\n",
    "command_job = command(\n",
    "    code=str(src_dir),\n",
    "    command=lora_command,\n",
    "    inputs={\"data\": train_input},\n",
    "    environment=registered_env,\n",
    "    compute=COMPUTE_NAME,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    display_name=\"qwen-vl-lora-train\",\n",
    "    outputs={\n",
    "        \"trained_lora\": Output(type=AssetTypes.URI_FOLDER, path=OUTPUT_PATH)\n",
    "    },\n",
    "    description=\"LoRA finetuning job for Qwen-VL\"\n",
    "),\n",
    "\n",
    "command_job.inputs[\"data\"].mode = \"mount\"\n",
    "submitted_job = ml_client.jobs.create_or_update(command_job)\n",
    "print(f\"Job submitted: {submitted_job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec429b9",
   "metadata": {},
   "source": [
    "## 监控训练日志\n",
    "\n",
    "运行以下代码实时查看作业日志，或直接在 Azure ML Studio 门户中监控。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39777670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在命令行实时查看训练过程输出\n",
    "ml_client.jobs.stream(submitted_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74edfe3",
   "metadata": {},
   "source": [
    "## 注册 LoRA 适配器模型\n",
    "\n",
    "作业完成后，可将输出目录注册为模型资产，方便后续部署或批量推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# 将 LoRA 输出路径注册为模型资产，便于后续部署或版本管理\n",
    "model_asset = Model(\n",
    "    name=\"qwen-vl-lora-adapter\",\n",
    "    path=submitted_job.outputs[\"trained_lora\"].uri,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"LoRA adapter fine-tuned from Qwen-VL\",\n",
    "    tags={\"base_model\": BASE_MODEL, \"task\": \"instruction-following\"}\n",
    ")\n",
    "registered_model = ml_client.models.create_or_update(model_asset)\n",
    "print(f\"Model registered: {registered_model.name}:{registered_model.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d908e",
   "metadata": {},
   "source": [
    "## 本地验证 LoRA 效果\n",
    "\n",
    "以下示例展示如何下载输出、合并 LoRA 权重并运行简单推理。请确保有可用 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "# 下载 Azure ML 作业输出，获取存放 LoRA 权重的本地临时目录\n",
    "download_dir = pathlib.Path(tempfile.mkdtemp(prefix=\"qwen-lora-\"))\n",
    "ml_client.jobs.download(submitted_job.name, output_name=\"trained_lora\", download_path=download_dir.as_posix())\n",
    "print(f\"Artifacts downloaded to {download_dir}\")\n",
    "\n",
    "# 加载基础模型并应用 LoRA 适配器\n",
    "adapter_path = next(download_dir.glob('**/outputs'), download_dir)\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(BASE_MODEL, trust_remote_code=True).to(\"cuda\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "\n",
    "# 准备用于测试的单张图片，构造对话式输入模板\n",
    "test_image_path = \"<local_image_path>\"  # 替换为本地图像\n",
    "test_image = Image.open(test_image_path).convert(\"RGB\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": test_image}, {\"type\": \"text\", \"text\": \"请描述这张图片。\"}]}\n",
    "]\n",
    "prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 生成回复并打印，便于快速验证效果\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
